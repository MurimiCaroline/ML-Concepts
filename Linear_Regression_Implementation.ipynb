{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWenjQ4JbybwSh81g2s3pC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MurimiCaroline/ML-Concepts/blob/master/Linear_Regression_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gMUlOr6E7NDq",
        "outputId": "c9bcd1e2-02ce-4762-dcbb-9c732359e371"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b2d6581bd484>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    orig_data = np.loadtxt(file_path,skiprows=1) # Ignore the title in the first row of the dataset.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sample_count = X.shape[0]\n",
        "# Calculate the gradient based on the matrix 1/m ∑(((h(x^i)-y^i)) x_j^i)\n",
        "return (1./sample_count)*X.T.dot(X.dot(theta)-y)\n",
        "def get_training_data(file_path):\n",
        "orig_data = np.loadtxt(file_path,skiprows=1) # Ignore the title in the first row of the dataset.\n",
        "cols = orig_data.shape[1]\n",
        "return (orig_data, orig_data[:, :cols - 1], orig_data[:, cols-1:])\n",
        "# Initialize the θ array.\n",
        "def init_theta(feature_count):\n",
        "return np.ones(feature_count).reshape(feature_count, 1)\n",
        "def gradient_descending(X, y, theta, alpha):\n",
        "Jthetas= [] # Record the change trend of the cost function J(θ) to confirm the gradient descent is correct.\n",
        "# Calculate the loss function, which is equal to the square of the difference between the actual value and the\n",
        "predicted value: (y^i-h(x^i))^2\n",
        "Jtheta = (X.dot(theta)-y).T.dot(X.dot(theta)-y)\n",
        "index = 0\n",
        "gradient = generate_gradient(X, theta, y) # Calculate the gradient.\n",
        "while not np.all(np.absolute(gradient) <= 1e-5): # End the calculation when the gradient is less than 0.00001.\n",
        "theta = theta - alpha * gradient\n",
        "gradient = generate_gradient(X, theta, y) # Calculate the new gradient.\n",
        "# Calculate the loss function, which is equal to the square of the difference between the actual value and\n",
        "the predicted value: (y^i-h(x^i))^2\n",
        "Jtheta = (X.dot(theta)-y).T.dot(X.dot(theta)-y)\n",
        "if (index+1) % 10 == 0:\n",
        "Jthetas.append((index, Jtheta[0])) # Record the result every 10 calculations.\n",
        "index += 1\n",
        "return theta,Jthetas\n",
        "# Plot the loss function change curve.\n",
        "def showJTheta(diff_value):\n",
        "p_x = []\n",
        "p_y = []\n",
        "for (index, sum) in diff_value:\n",
        "p_x.append(index)\n",
        "p_y.append(sum)\n",
        "plt.plot(p_x, p_y, color='b')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('loss funtion')\n",
        "plt.title('step - loss function curve')\n",
        "plt.show()\n",
        "# Plot the actual data points and the fitted curve.\n",
        "def showlinercurve(theta, sample_training_set):\n",
        "  x, y = sample_training_set[:, 1], sample_training_set[:, 2]\n",
        "z = theta[0] + theta[1] * x\n",
        "plt.scatter(x, y, color='b', marker='x',label=\"sample data\")\n",
        "plt.plot(x, z, 'r', color=\"r\",label=\"regression curve\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('liner regression curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Read the dataset.\n",
        "training_data_include_y, training_x, y = get_training_data(\"./ML/02/lr2_data.txt\")\n",
        "# Obtain the numbers of samples and features, respectively.\n",
        "sample_count, feature_count = training_x.shape\n",
        "# Define the learning step α.\n",
        "alpha = 0.01\n",
        "# Initialize θ.\n",
        "theta = init_theta(feature_count)\n",
        "# Obtain the final parameter θ and cost.\n",
        "result_theta,Jthetas = gradient_descending(training_x, y, theta, alpha)\n",
        "# Display the parameter.\n",
        "print(\"w:{}\".format(result_theta[0][0]),\"b:{}\".format(result_theta[1][0]))\n",
        "showJTheta(Jthetas)\n",
        "showlinercurve(result_theta, training_data_include_y)"
      ]
    }
  ]
}